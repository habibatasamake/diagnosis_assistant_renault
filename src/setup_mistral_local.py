import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

import torch

# ğŸ“ Dossier local pour stocker le modÃ¨le
local_dir = os.path.expanduser("~/Documents/LLM_model")
os.makedirs(local_dir, exist_ok=True)
print(f"ğŸ“‚ Dossier du modÃ¨le : {local_dir}")

# ğŸ”§ Configuration pour quantification 4-bit (nf4) avec bitsandbytes
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

# ğŸ§  ID du modÃ¨le Ã  tÃ©lÃ©charger
model_id = "mistralai/Mistral-7B-Instruct-v0.2"

# ğŸ“¥ TÃ©lÃ©charger et sauvegarder le tokenizer
print("â³ TÃ©lÃ©chargement du tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.save_pretrained(local_dir)

# ğŸ“¥ TÃ©lÃ©charger et sauvegarder le modÃ¨le (peut prendre quelques minutes)
print("â³ TÃ©lÃ©chargement du modÃ¨le quantifiÃ© 4-bit (nf4)...")
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True,
    quantization_config=quant_config
)
model.save_pretrained(local_dir)

print("\nâœ… ModÃ¨le Mistral-7B-Instruct tÃ©lÃ©chargÃ© et prÃªt Ã  l'usage local.")
print(f"ğŸ“Œ Utilisez ce chemin pour charger le modÃ¨le : '{local_dir}'")
